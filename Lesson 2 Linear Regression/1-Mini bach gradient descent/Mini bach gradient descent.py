import numpy as np
# Setting a random seed, feel free to change it and see different solutions.
np.random.seed(42)
import matplotlib.pyplot as plt

'''
MSEStep function implements the gradient descent step for squared error as a performance metric.

X:array of predictor features 
y_predicotor: array of outcome values 
W:prediction feature coefficient 
b:prediction function intercept
W_new : predictor feature coefficients following gradient descent step
b_new : intercept following gradient descent step
using numpy library to multiplay multi-dimenstional arrays and matrices 
'''

def MSEStep(X,y,W,b,learn_rate=0.005):
    y_predictor=np.matmul(X,W)+b       
    error=y-y_predictor
    W_new=W+learn_rate*np.matmul(error,X)    
    b_new=b+learn_rate*error.sum()
    return W_new,b_new


'''
miniBatchGD function performs mini-batch gradient descent on a given dataset.

n_points: number of points 
X : array of predictor features
y : array of outcome values
batch_size : how many data points will be sampled for each iteration
learn_rate : learning rate
num_iter : number of batches used
regression_coef : array of slopes and intercepts generated by gradient descent procedure
    
'''
def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):
    n_points = X.shape[0]
    W = np.zeros(X.shape[1]) # coefficients
    b = 0 # intercept
    
    # run iterations
    regression_coef = [np.hstack((W,b))]
    for _ in range(num_iter):
        batch = np.random.choice(range(n_points), batch_size) #choose randomly from 100 point(total number of points)---> 20 points 
        X_batch = X[batch,:]
        y_batch = y[batch]
        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)
        regression_coef.append(np.hstack((W,b)))
    
    return regression_coef


if __name__ == "__main__":
    # perform gradient descent
    data = np.loadtxt('data.csv', delimiter = ',')
    X = data[:,:-1]
    y = data[:,-1]
    regression_coef = miniBatchGD(X, y)
    
    # plot the results
    import matplotlib.pyplot as plt
    
    plt.figure()
    X_min = X.min()
    X_max = X.max()
    counter = len(regression_coef)
    for W, b in regression_coef:
        counter -= 1
        color = [1 - 0.92 ** counter for _ in range(3)]
        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)
    plt.scatter(X, y, zorder = 3)
    plt.show()

